---
layout: none
category: "Engineering"
within_category_ix: 0
title:  "Fusing attention scaling"
---

https://twitter.com/Birchlabs/status/1588703520433541120?s=20

<p>
  <small>I sped up LLaMA_MPS <a href="https://github.com/jankais3r/LLaMA_MPS/pull/5">by <strong>4.7%</strong></a> with this same technique.</small>
</p>