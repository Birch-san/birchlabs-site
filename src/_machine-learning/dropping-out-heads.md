---
layout: none
category: "Explorations"
within_category_ix: 50
title:  "Dropping out attention heads"
---

<p>
  I was discussing <a href="https://arxiv.org/abs/1911.02150">One Write-Head is All You Need</a> with Aditya Ramesh (OpenAI), who explained that diffusion models sometimes learn to drop out attention heads.
</p>
<p>
  I wondered whether stable-diffusion was doing this. If that were the case: perhaps we could verify it by replacing an attention head's weights with weights copied from one of its siblings. If this did not result in any change: perhaps it would indicate that the attention head wasn't being used.
</p>
<p>
  Depending on how many/which heads I dropped out: I found that it didn't completely destroy the image. In some cases I still got a perfectly usable image.
</p>
<p>
  I'd love to see a large-scale model evaluate the One Write-Head paper. It's a bit harder now that we're accustomed to Flash Attention - a bespoke CUDA kernel or Triton implementation would need to be created to get performance parity.
</p>
<ul>
  <li><a href="https://twitter.com/Birchlabs/status/1609592913947955200">Good result: Dropping out one cross-attention value head</a></li>
  <li><a href="https://twitter.com/Birchlabs/status/1609592913947955200">Tenuous result: Dropping out one cross-attention value head</a></li>
</ul>