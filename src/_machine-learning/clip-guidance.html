---
layout: none
category: "Technique Implementations"
within_category_ix: 0
title:  "CLIP-guided diffusion"
---

<figure class="table-fig center-fig article-fig">
  <table>
    <thead>
      <tr>
        <th>Standard</th>
        <th>CLIP-guided</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>
          <a href="/assets/machine-learning/clip-guidance/standard-cfg.png">
            <img src="/assets/machine-learning/clip-guidance/standard-cfg.png" width="256px" height="256px" loading="lazy">
          </a>
        </td>
        <td>
          <a href="/assets/machine-learning/clip-guidance/clip-guided-cfg.png">
            <img src="/assets/machine-learning/clip-guidance/clip-guided-cfg.png" width="256px" height="256px" loading="lazy">
          </a>
        </td>
      </tr>
    </tbody>
  </table>
  <figcaption>Using LAION CLIP guidance to steer diffusion towards producing a red, ribboned bear</figcaption>
</figure>

<p>
  I implemented <a href="https://arxiv.org/abs/2103.00020"><abbr title="Contrastive Languageâ€“Image Pre-training">CLIP</abbr></a>-guided diffusion as a <a href="https://github.com/crowsonkb/k-diffusion/blob/master/sample_clip_guided.py">k-diffusion wrapper</a> for CompVis stable-diffusion.
</p>
<p>
  A particular challenge was getting it to work on Mac. Reducing the batch size (by disabling <abbr title="classifier-free guidance">CFG</abbr>) fixed it.<br>
  I later found that CFG could be enabled, so long as the cond and uncond were submitted to the Unet in separate batches.
</p>
<p>
  Thanks to <a href="https://twitter.com/wattmaller1">Matt Waller</a> for sharing the tip of single-cond batches on Mac, who found the same trick fixed bugs in early CoreML models.
</p>
<ul>
  <li><a href="https://twitter.com/Birchlabs/status/1578141960249876482">Twitter thread: without CFG</a></li>
  <li><a href="https://twitter.com/Birchlabs/status/1591214403575287808">Twitter thread: with CFG</a></li>
</ul>